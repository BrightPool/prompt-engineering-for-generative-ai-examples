{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imports:\n",
    "from content_collection import collect_serp_data_and_extract_text_from_webpages\n",
    "from custom_summarize_chain import create_all_summaries, DocumentSummary\n",
    "from expert_interview_chain import InterviewChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-search-results pandas html2text pytest-playwright chromadb nest_asyncio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant variables:\n",
    "TOPIC = \"Neural networks\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_y/20jl658s4jl0zvy5c0x0c5140000gn/T/ipykernel_59331/796634559.py:2: RuntimeWarning: coroutine 'collect_serp_data_and_extract_text_from_webpages' was never awaited\n",
      "  text_documents = await collect_serp_data_and_extract_text_from_webpages(topic=TOPIC)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Extract content from webpages into LangChain documents:\n",
    "text_documents = await collect_serp_data_and_extract_text_from_webpages(topic=TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM, text splitter + parser:\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1500, chunk_overlap=400\n",
    ")\n",
    "parser = PydanticOutputParser(pydantic_object=DocumentSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing the data!\n",
      "Summarizing the data!\n",
      "Summarizing the data!\n"
     ]
    }
   ],
   "source": [
    "summaries = await create_all_summaries(text_documents, parser, llm, text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Expert Interview Questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_chain = InterviewChain(topic=TOPIC, document_summaries=summaries)\n",
    "interview_questions = interview_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer The Interview Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question: Can you explain the concept of backpropagation and its role in training neural networks?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: What are some of the challenges or limitations of neural networks?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: How do neural networks differ from other machine learning algorithms?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: What are some real-world applications of neural networks?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: What are the current trends or advancements in neural network research?\n",
      "\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for question in interview_questions.questions:\n",
    "    print(f\"Answer the following question: {question.question}\\n\", flush=True)\n",
    "    answer = input(f\"Answer the following question: {question.question}\\n\")\n",
    "    print('------------------------------------------')\n",
    "    question.answer = answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## General Article Outline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from article_outline_generation import BlogOutlineGenerator\n",
    "blog_outline_generator = BlogOutlineGenerator(topic=TOPIC, questions_and_answers=[item.dict()  \n",
    "                                                                                  for item in interview_questions.questions ] )\n",
    "questions_and_answers = blog_outline_generator.questions_and_answers\n",
    "outline_result = blog_outline_generator.generate_outline(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Article Text Generation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x7fe76088d790>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from article_generation import ContentGenerator\n",
    "content_gen = ContentGenerator(topic=TOPIC, outline=outline_result, questions_and_answers=questions_and_answers)\n",
    "content_gen.split_and_vectorize_documents(text_documents)\n",
    "blog_post = content_gen.generate_blog_post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"# An Introduction to Neural Networks\\n\\nNeural networks have become a fundamental component of artificial intelligence (AI) and are at the heart of deep learning algorithms. In this section, we will explore what neural networks are and how they work.\\n\\n## 1. What are Neural Networks?\\n\\nNeural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are computational models inspired by the structure and functioning of the human brain. They consist of interconnected nodes, or artificial neurons, that process and transform data.\\n\\nThe structure of a neural network is organized into layers: an input layer, one or more hidden layers, and an output layer. Each node in the network is connected to others and has an associated weight and threshold. These weights determine the importance of the input data, while the threshold determines whether the neuron activates and passes data to the next layer.\\n\\nNeural networks learn and improve their accuracy over time through a process called training. During training, the network is exposed to labeled data, and it adjusts its weights and thresholds to minimize the difference between its predicted outputs and the true labels. This process is often guided by a supervised learning algorithm, such as backpropagation.\\n\\nWhile neural networks require large amounts of data for effective training, they offer powerful capabilities in various domains. They can classify and cluster data, perform speech and image recognition tasks, and even power search algorithms like Google's.\\n\\nNeural networks are not without their challenges and limitations. Some of these include the need for extensive labeled data, the risk of overfitting, the interpretability of deep neural networks, and the computational requirements for training. Additionally, concerns regarding bias, fairness, adversarial attacks, and environmental impact have also been raised.\\n\\nDespite these challenges, neural networks have found numerous real-world applications. They enable advancements in image and speech recognition, drive innovations in fields such as healthcare and personalized content recommendations, and are at the forefront of cutting-edge research in AI.\\n\\nIn the next section, we will delve into the components that make up neural networks and explore their role in the network's functioning.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 1](link-to-insight-1)\\n- [Insight 3](link-to-insight-3)\\n- [Insight 4](link-to-insight-4)\",\n",
       " '## 2. Components of Neural Networks\\n\\nNeural networks consist of several key components that work together to process and transform data. Understanding these components is crucial for gaining insight into how neural networks function. Let\\'s explore the main components of neural networks:\\n\\n1. Neurons (Nodes): The fundamental building blocks of neural networks are artificial neurons, also known as nodes. Each node receives input data, performs computations, and passes the transformed data to the next layer. These nodes mimic the behavior of biological neurons in the human brain.\\n\\n2. Layers: Neural networks are organized into layers, each serving a specific purpose. The three main types of layers are:\\n\\n   - Input Layer: The input layer receives the initial data and passes it to the next layer.\\n   \\n   - Hidden Layers: Hidden layers are located between the input and output layers. They process and transform the data, extracting important features and patterns. The number of hidden layers and the number of nodes in each layer can vary depending on the complexity of the problem.\\n   \\n   - Output Layer: The output layer produces the final result or prediction based on the processed data from the hidden layers. The number of nodes in the output layer depends on the type of problem the neural network is designed to solve.\\n\\n3. Weights and Thresholds: Each connection between nodes in a neural network is associated with a weight. These weights determine the strength or importance of the input data. Additionally, each node has a threshold, also known as a bias, which determines whether the node activates and passes data to the next layer.\\n\\n4. Activation Function: The activation function determines the output of a node based on the weighted sum of its inputs. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and tanh (hyperbolic tangent). The choice of activation function depends on the specific problem and the desired behavior of the network.\\n\\n5. Connections: Nodes in a neural network are connected to each other through connections. These connections enable the flow of data and allow information to propagate through the network during the forward pass.\\n\\n6. Training Algorithm: Neural networks learn and improve their performance through a process called training. During training, the network is exposed to labeled data, and it adjusts the weights and thresholds to minimize the difference between its predicted outputs and the true labels. The choice of training algorithm, such as backpropagation, plays a crucial role in updating the weights and thresholds.\\n\\n7. Backpropagation: Backpropagation is a widely used training algorithm in neural networks. It calculates the gradient of the loss function with respect to each weight and uses this information to update the weights in the network. This iterative process allows the network to learn from its mistakes and improve its predictions.\\n\\nThese components work in harmony to process and analyze data, enabling neural networks to make predictions and perform various tasks. By understanding the components of neural networks, we can appreciate the complexity and power of these AI models.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 1](link-to-insight-1) (Question: \"Can you explain the concept of backpropagation and its role in training neural networks?\")\\n- [Insight 3](link-to-insight-3) (Question: \"How do neural networks differ from other machine learning algorithms?\")',\n",
       " \"## 3. Training Neural Networks\\n\\nTraining neural networks is a crucial step in their development and involves optimizing the network's weights and thresholds to improve its accuracy. In this section, we will explore the training process and the role of backpropagation in training neural networks.\\n\\nDuring the training process, all the weights and thresholds of a neural network are initially set to random values. The network is then presented with training data, which flows through the network from the input layer to the output layer. The data is transformed and processed in complex ways as it passes through the layers.\\n\\nThe goal of training is to adjust the weights and thresholds in such a way that the network produces outputs that closely match the true labels of the training data. This is achieved by comparing the predicted outputs of the network with the true labels and calculating an error or loss value.\\n\\nOne commonly used algorithm for training neural networks is backpropagation. Backpropagation is a supervised learning algorithm that calculates the gradient of the loss function with respect to each weight in the network. It does this by applying the chain rule of calculus, which allows the error to be attributed to each neuron in the network.\\n\\nOnce the gradients have been calculated, the weights and thresholds are updated using an optimization algorithm, such as stochastic gradient descent (SGD). The optimization algorithm adjusts the weights and thresholds in the direction that minimizes the loss, gradually improving the network's performance.\\n\\nThe training process is typically iterative, with multiple passes through the training data. Each pass is called an epoch. During each epoch, the network's weights and thresholds are adjusted based on the gradients calculated from a subset of the training data, known as a mini-batch. Using mini-batches helps to speed up the training process and improve generalization.\\n\\nIt is important to mention that training a neural network can be computationally intensive and time-consuming, especially for deep networks with large datasets. High-end GPUs or TPUs are often used to accelerate the training process.\\n\\nDespite the challenges and limitations of training neural networks, such as the need for extensive labeled data and the risk of overfitting, they have proven to be powerful tools in various domains. They have enabled breakthroughs in image and speech recognition, natural language processing, and personalized content recommendations.\\n\\nIn conclusion, training neural networks involves adjusting the weights and thresholds of the network to minimize the difference between predicted outputs and true labels. Backpropagation is a commonly used algorithm for calculating the gradients and updating the weights and thresholds. The training process is iterative and computationally intensive but has led to significant advancements in AI applications.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 1](link-to-insight-1)\\n- [Insight 3](link-to-insight-3)\",\n",
       " \"# 4. Types of Neural Networks\\n\\nNeural networks can be classified into different types, each serving specific purposes and catering to different use cases. While this is not an exhaustive list, below are some of the most common types of neural networks that you'll come across:\\n\\n1. Perceptron:\\n   - The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958.\\n   - It consists of a single layer of artificial neurons, also known as perceptrons.\\n   - The perceptron is primarily used for binary classification tasks, where it learns to classify inputs into two categories.\\n\\n2. Feedforward Neural Networks (Multi-layer Perceptrons or MLPs):\\n   - Feedforward neural networks are the most commonly used neural networks.\\n   - They consist of an input layer, one or more hidden layers, and an output layer.\\n   - Each neuron in the network is connected to neurons in the subsequent layer, and data flows only in the forward direction.\\n   - MLPs are particularly effective at solving complex problems and can model non-linear relationships in data.\\n   - They are the foundation for various applications such as computer vision, natural language processing, and speech recognition.\\n\\n3. Convolutional Neural Networks (CNNs):\\n   - CNNs are specialized neural networks designed for image processing and recognition tasks.\\n   - They employ convolutional layers to detect local patterns and features in images.\\n   - CNNs are known for their ability to automatically extract hierarchical representations from images, allowing them to achieve state-of-the-art results in tasks such as image classification and object detection.\\n\\n4. Recurrent Neural Networks (RNNs):\\n   - RNNs are designed to handle sequential data, such as time series or natural language data.\\n   - They have feedback connections that allow information to flow in a loop, enabling them to capture temporal dependencies.\\n   - RNNs are well-suited for tasks like speech recognition, language modeling, machine translation, and sentiment analysis.\\n\\n5. Long Short-Term Memory (LSTM) Networks:\\n   - LSTMs are a variant of RNNs that address the vanishing gradient problem and can capture long-term dependencies in sequential data.\\n   - They use memory cells and specialized gates to control the flow of information, allowing them to retain and update information over long sequences.\\n   - LSTMs are commonly used in tasks involving speech recognition, language translation, and handwriting recognition.\\n\\n6. Generative Adversarial Networks (GANs):\\n   - GANs consist of two neural networks: a generator and a discriminator.\\n   - The generator network generates synthetic data, while the discriminator network tries to distinguish between real and synthetic data.\\n   - GANs are used for tasks such as image synthesis, image-to-image translation, and text generation.\\n\\nThese are just a few examples of the types of neural networks that exist. Each type has its own strengths and applications, and researchers are continually exploring new variations and architectures to tackle different challenges.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 1](link-to-insight-1)\\n- [Insight 2](link-to-insight-2)\",\n",
       " '## 5. History of Neural Networks\\n\\nThe history of neural networks is longer than most people think. While the idea of \"a machine that thinks\" can be traced back to the Ancient Greeks, the evolution of thinking around neural networks has ebbed and flowed in popularity over the years. Let\\'s explore some key milestones in the history of neural networks:\\n\\n- **1943:** Warren S. McCulloch and Walter Pitts published \"A logical calculus of the ideas immanent in nervous activity.\" This research aimed to understand how the human brain produces complex patterns through interconnected neurons. One of the main ideas that emerged from this work was the comparison of neurons with a binary threshold to Boolean logic.\\n\\n- **1958:** Frank Rosenblatt\\'s research on the perceptron marked a significant advancement in neural network development. In his work, \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain,\" Rosenblatt introduced weights to the equation, taking McCulloch and Pitt\\'s work further. He demonstrated the ability of a computer, specifically an IBM 704, to learn how to distinguish cards marked on the left from those marked on the right.\\n\\n- **1974:** Paul Werbos, in his PhD thesis, was the first person in the US to note the application of backpropagation within neural networks. Backpropagation is a supervised learning algorithm used for training neural networks. It calculates the gradient of the loss function with respect to each weight and updates the weights to minimize the loss.\\n\\nThese early contributions laid the foundation for contemporary neural networks and set the stage for further advancements in the field. Neural network theory has helped improve our understanding of how neurons function in the brain, serving as the basis for efforts to create artificial intelligence.\\n\\nIt\\'s important to note that the theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). They both recognized that thoughts and body activity result from interactions among neurons within the brain. Despite skepticism from the scientific community at the time, their theories paved the way for future research.\\n\\nSince then, neural networks have continued to evolve, fueled by advancements in computing power, availability of large datasets, and innovative algorithms. Between 2009 and 2012, for example, recurrent neural networks (RNNs) and deep feedforward neural networks developed by JÃ¼rgen Schmidhuber\\'s research group at the Swiss AI Lab IDSIA won several international competitions in pattern recognition and machine learning.\\n\\nIn recent years, there have been notable trends and advancements in neural network research:\\n\\n- Transformers and Attention Mechanisms: Originally designed for natural language processing tasks, transformer architectures have shown promise across various domains, including computer vision and bioinformatics.\\n\\n- Self-supervised Learning: This approach aims to learn representations from the data itself by predicting certain parts of the data from other parts, reducing the reliance on labeled datasets.\\n\\n- Neural Architecture Search (NAS): Using machine learning, NAS automates the process of finding the best network architecture for a specific task, saving time and effort.\\n\\n- Federated Learning: This decentralized approach to training allows models to learn from data localized on multiple devices or servers, addressing data privacy concerns.\\n\\n- Explainable AI (XAI): As neural networks become more complex, there is a growing need for interpretability and transparency. XAI focuses on making deep learning models easier to understand and explain.\\n\\n- Energy-efficient Networks: With the environmental impact of training large models in mind, research is being conducted on smaller, more efficient models and specialized hardware.\\n\\n- Capsule Networks: Proposed as an alternative to convolutional networks, capsule networks aim to encode spatial hierarchies between features, potentially improving generalization.\\n\\n- Continuous Learning and Neural Plasticity: Allowing neural networks to learn new tasks while retaining knowledge from previous tasks, addressing the \"catastrophic forgetting\" problem.\\n\\n- Cross-modal Learning: Integrating information from multiple data types or sensory modalities, such as vision and sound, to make more informed predictions.\\n\\nThese advancements demonstrate the ongoing progress and potential of neural network research.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 1](link-to-insight-1)\\n- [Insight 2](link-to-insight-2)\\n- [Insight 3](link-to-insight-3)\\n- [Insight 4](link-to-insight-4)',\n",
       " '# 6. Challenges and Limitations of Neural Networks\\n\\nNeural networks have garnered significant attention and achieved remarkable success in various domains. However, they also come with their fair share of challenges and limitations. It is essential to understand these limitations to ensure the responsible and effective use of neural networks. Let\\'s explore some of the key challenges and limitations:\\n\\n1. Data Requirement:\\n   - Neural networks often require large amounts of labeled data for effective training.\\n   - Acquiring and labeling vast amounts of data can be challenging and time-consuming.\\n   - Limited availability of labeled data can hinder the performance and generalization of neural networks.\\n\\n2. Overfitting:\\n   - Neural networks, especially with a large number of parameters, are prone to overfitting.\\n   - Overfitting occurs when the network becomes too specialized in learning the training data and fails to generalize to new, unseen data.\\n   - Regularization techniques, such as dropout and weight decay, can help mitigate overfitting.\\n\\n3. Interpretability:\\n   - Deep neural networks, with their complex architectures, are often considered \"black boxes.\"\\n   - Understanding how and why a neural network makes specific predictions or decisions can be challenging.\\n   - The lack of interpretability can be problematic in critical domains such as healthcare and finance, where explanations are necessary.\\n\\n4. Training Time and Computational Requirements:\\n   - Training neural networks, especially large and deep models, can be computationally intensive and time-consuming.\\n   - High-end GPUs or TPUs are often required to accelerate the training process.\\n   - The computational requirements can limit the accessibility and practicality of training neural networks in resource-constrained environments.\\n\\n5. Local Minima and Gradient Issues:\\n   - Neural network training algorithms can sometimes get stuck in local minima, leading to suboptimal solutions.\\n   - In deep networks, gradients can vanish or explode, making the training process challenging.\\n   - Advanced optimization techniques, careful initialization, and network architectures like LSTMs and residual networks have partially addressed these issues.\\n\\n6. Bias and Fairness:\\n   - Neural networks are trained on historical data, which may contain biases present in society.\\n   - If not handled carefully, neural networks can perpetuate or even amplify biases, leading to fairness concerns.\\n   - Efforts are being made to develop techniques and frameworks for fair and unbiased neural network training.\\n\\n7. Generalization Concerns:\\n   - Neural networks can sometimes memorize the training data, including outliers and noise, rather than learning the underlying patterns.\\n   - This can lead to poor generalization and performance on new, unseen data.\\n   - Techniques such as regularization, data augmentation, and transfer learning can help improve generalization.\\n\\n8. Adversarial Attacks:\\n   - Neural networks are vulnerable to adversarial attacks, where maliciously crafted inputs can deceive the network into making incorrect predictions.\\n   - Adversarial robustness techniques, such as adversarial training and defensive distillation, are being developed to enhance the security of neural networks.\\n\\n9. Model Size and Deployment Constraints:\\n   - State-of-the-art neural network models can have billions of parameters, leading to storage and memory constraints during deployment.\\n   - Deploying large models on resource-constrained devices, such as mobile phones or embedded systems, can be challenging.\\n\\n10. Environmental Impact:\\n    - The computational requirements for training large neural networks have a significant energy consumption footprint.\\n    - The environmental impact of energy-intensive training processes is a concern and calls for energy-efficient training methods and hardware.\\n\\n11. Hyperparameter Tuning:\\n    - Neural networks have various hyperparameters, such as learning rate, batch size, and network architecture, that need to be carefully tuned.\\n    - Finding the optimal combination of hyperparameters can be a time-consuming and iterative process.\\n\\nIt is important to note that ongoing research and advancements are continuously addressing these challenges and limitations. Researchers are exploring techniques like self-supervised learning, explainable AI, energy-efficient networks, and neurosymbolic integration to overcome these limitations and push the boundaries of neural network capabilities.\\n\\nBy understanding and addressing these challenges, we can harness the power of neural networks while being mindful of their limitations, ensuring responsible and effective use in real-world applications.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 1](link-to-insight-1)\\n- [Insight 2](link-to-insight-2)\\n- [Insight 3](link-to-insight-3)\\n- [Insight 4](link-to-insight-4)',\n",
       " '# 7. Real-world Applications of Neural Networks\\n\\nNeural networks have found numerous applications across various industries and domains. Their ability to learn from data and make accurate predictions has led to significant advancements in a wide range of fields. Let\\'s explore some real-world applications of neural networks:\\n\\n1. Image and Speech Recognition:\\n   - Neural networks power advancements in image recognition, enabling applications such as facial recognition, object detection, and image classification.\\n   - Speech recognition systems, like voice assistants and speech-to-text converters, utilize neural networks to understand and process spoken language.\\n\\n2. Healthcare:\\n   - Neural networks have revolutionized healthcare by assisting in disease identification and diagnosis from medical images, such as MRIs and CT scans.\\n   - They can predict patient outcomes, aid in drug discovery, and assist in personalized treatment planning.\\n\\n3. Natural Language Processing (NLP):\\n   - Neural networks are at the forefront of NLP, enabling applications like language translation, sentiment analysis, and chatbots.\\n   - They can understand and generate human-like text, making it easier to process and analyze large volumes of textual data.\\n\\n4. Recommendation Systems:\\n   - Neural networks power personalized recommendation systems in entertainment and e-commerce platforms.\\n   - They analyze user behavior and preferences to provide tailored recommendations for movies, music, products, and more.\\n\\n5. Autonomous Vehicles:\\n   - Neural networks play a crucial role in self-driving cars, enabling perception, object detection, and decision-making capabilities.\\n   - They analyze sensor data from cameras, lidar, and radar to understand the surrounding environment and make informed driving decisions.\\n\\n6. Financial Applications:\\n   - Neural networks are used in various financial applications, such as fraud detection, credit scoring, and stock market prediction.\\n   - They analyze patterns and historical data to identify anomalies, assess creditworthiness, and make predictions about financial markets.\\n\\n7. Robotics:\\n   - Neural networks are employed in robotics to enhance perception, control, and decision-making capabilities.\\n   - They enable robots to navigate their environment, recognize objects, and perform complex tasks with precision.\\n\\n8. Gaming:\\n   - Neural networks have been used in gaming to develop intelligent agents capable of playing games like chess, Go, and poker at a high level.\\n   - They learn from experience and optimize strategies to compete against human players or other AI agents.\\n\\nThese are just a few examples of the real-world applications of neural networks. The versatility and power of neural networks make them valuable tools across diverse industries and fields, enabling advancements and innovations that were once considered science fiction.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 4](link-to-insight-4) (Question: \"What are some real-world applications of neural networks?\")',\n",
       " '# 8. Recent Trends and Advancements in Neural Network Research\\n\\nNeural network research has seen significant advancements in recent years, driven by the increasing availability of computational power, the abundance of data, and innovative algorithmic approaches. Here are some of the recent trends and advancements that have shaped the field of neural networks:\\n\\n1. Transformers and Attention Mechanisms:\\n   - Originally developed for natural language processing (NLP) tasks, transformer architectures and attention mechanisms have gained widespread attention.\\n   - Transformers, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), have achieved state-of-the-art results in various NLP tasks.\\n   - The attention mechanism allows the model to focus on relevant parts of the input, improving its ability to capture long-range dependencies and handle sequences of varying lengths.\\n   - Beyond NLP, transformer architectures have shown promise in computer vision and other domains.\\n\\n2. Self-supervised Learning:\\n   - Self-supervised learning has emerged as an alternative to supervised learning, reducing the reliance on large amounts of labeled data.\\n   - In self-supervised learning, neural networks learn to predict certain parts of the data from other parts, effectively creating their own labels.\\n   - By leveraging the inherent structure or redundancy in the data, self-supervised learning enables the training of deep models with limited labeled data.\\n\\n3. Neural Architecture Search (NAS):\\n   - Designing an optimal neural network architecture for a specific task can be challenging and time-consuming.\\n   - Neural Architecture Search (NAS) automates the process of finding the best network architecture by using machine learning techniques.\\n   - NAS explores a search space of possible architectures and selects the most promising ones based on their performance on a validation set.\\n   - This approach has led to the discovery of novel network architectures that outperform manually designed models in various domains.\\n\\n4. Federated Learning:\\n   - Federated Learning addresses privacy concerns by training models on decentralized data.\\n   - Instead of centralizing data on a single server, federated learning allows models to be trained on multiple devices or servers while keeping the data localized.\\n   - This approach enables organizations to collaboratively train models without sharing sensitive data, making it suitable for applications in healthcare, finance, and other domains.\\n\\n5. Transfer Learning and Few-shot Learning:\\n   - Transfer learning and few-shot learning techniques aim to leverage knowledge learned from one task or domain to improve performance on another, especially when data is limited.\\n   - Pretrained models, such as those trained on large-scale datasets like ImageNet, are fine-tuned on specific tasks, enabling faster and more accurate training on smaller datasets.\\n   - Few-shot learning techniques focus on training models with limited labeled examples, simulating scenarios where only a few labeled examples are available for a new task.\\n\\n6. Explainable AI (XAI):\\n   - As neural networks become more complex, understanding their decision-making processes has become a critical challenge.\\n   - Explainable AI (XAI) aims to make neural networks more interpretable and transparent, enabling users to understand why a model makes certain predictions.\\n   - XAI methods include attention visualization, saliency maps, and rule extraction techniques to provide insights into the internal workings of neural networks.\\n\\n7. Energy-efficient Networks:\\n   - The environmental impact of training large neural networks has led to increased research into energy-efficient models and hardware.\\n   - Efficient network architectures and training techniques aim to reduce the computational resources required for training and inference, making deep learning more sustainable.\\n   - Techniques such as model pruning, quantization, and knowledge distillation enable the deployment of compact models with minimal loss in performance.\\n\\n8. Capsule Networks:\\n   - Capsule networks are an alternative to convolutional neural networks (CNNs) that aim to capture spatial hierarchies between features more effectively.\\n   - Capsule networks use groups of neurons, called capsules, to represent specific features and their spatial relationships.\\n   - This hierarchical representation allows capsule networks to better handle variations in object pose, occlusion, and other complex visual factors.\\n\\nThese recent trends and advancements in neural network research have pushed the boundaries of what is possible with AI, enabling breakthroughs in natural language processing, computer vision, and other domains. As research continues to evolve, we can expect further innovations that will shape the future of neural networks.\\n\\n## References:\\n- [Document 1](link-to-document-1)\\n- [Document 2](link-to-document-2)\\n- [Insight 4](link-to-insight-4)']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Title Tag Optimization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thumb in /Users/jamesaphoenix/Desktop/projects/clients/thumb/src (0.1)\n",
      "Requirement already satisfied: gradio in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from thumb) (3.41.2)\n",
      "Requirement already satisfied: langchain in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from thumb) (0.0.274)\n",
      "Requirement already satisfied: ipython in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from thumb) (7.31.1)\n",
      "Requirement already satisfied: ipywidgets in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from thumb) (7.6.5)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (9.2.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (2.11.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (3.5.2)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (5.1.1)\n",
      "Requirement already satisfied: pydub in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.25.1)\n",
      "Requirement already satisfied: gradio-client==0.5.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.5.0)\n",
      "Requirement already satisfied: fastapi in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.94.0)\n",
      "Requirement already satisfied: python-multipart in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.0.6)\n",
      "Requirement already satisfied: packaging in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (21.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (4.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.16.4)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (23.2.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (10.4)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (6.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.21.1)\n",
      "Requirement already satisfied: requests~=2.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (2.28.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (6.0.1)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (3.9.5)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (2.0.1)\n",
      "Requirement already satisfied: httpx in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.23.3)\n",
      "Requirement already satisfied: ffmpy in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (0.3.1)\n",
      "Requirement already satisfied: numpy~=1.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (1.24.4)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (2.0.3)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (2.10.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio->thumb) (1.10.12)\n",
      "Requirement already satisfied: fsspec in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from gradio-client==0.5.0->gradio->thumb) (2023.5.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (63.4.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (4.8.0)\n",
      "Requirement already satisfied: pygments in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (2.11.2)\n",
      "Requirement already satisfied: appnope in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (0.1.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (5.1.1)\n",
      "Requirement already satisfied: backcall in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipython->thumb) (4.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->thumb) (5.5.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->thumb) (6.15.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->thumb) (3.5.2)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->thumb) (1.0.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->thumb) (0.2.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from langchain->thumb) (0.5.7)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from langchain->thumb) (0.0.27)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from langchain->thumb) (4.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from langchain->thumb) (2.0.15)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from langchain->thumb) (3.8.4)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from langchain->thumb) (2.8.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from langchain->thumb) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thumb) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thumb) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thumb) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thumb) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thumb) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thumb) (1.3.1)\n",
      "Requirement already satisfied: toolz in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->thumb) (0.11.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->thumb) (4.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->thumb) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->thumb) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->thumb) (0.8.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio->thumb) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio->thumb) (3.6.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio->thumb) (3.8.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->thumb) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->thumb) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->thumb) (1.5.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->thumb) (1.5.5)\n",
      "Requirement already satisfied: psutil in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->thumb) (5.9.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->thumb) (7.3.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython->thumb) (0.8.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->thumb) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->thumb) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->thumb) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->thumb) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->thumb) (1.4.2)\n",
      "Requirement already satisfied: fastjsonschema in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->thumb) (2.16.2)\n",
      "Requirement already satisfied: jupyter_core in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->thumb) (4.11.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->thumb) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->thumb) (2022.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython->thumb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->thumb) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from requests~=2.0->gradio->thumb) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from requests~=2.0->gradio->thumb) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from requests~=2.0->gradio->thumb) (3.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain->thumb) (2.0.2)\n",
      "Requirement already satisfied: click>=7.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from uvicorn>=0.14.0->gradio->thumb) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from uvicorn>=0.14.0->gradio->thumb) (0.14.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from widgetsnbextension~=3.5.0->ipywidgets->thumb) (6.4.12)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from fastapi->gradio->thumb) (0.26.0.post1)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from httpx->gradio->thumb) (1.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from httpx->gradio->thumb) (1.2.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from httpx->gradio->thumb) (0.16.3)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio->thumb) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->thumb) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->thumb) (0.4)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (1.8.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (6.4.4)\n",
      "Requirement already satisfied: prometheus-client in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.14.1)\n",
      "Requirement already satisfied: argon2-cffi in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (21.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio->thumb) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->thumb) (0.4.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.5.13)\n",
      "Requirement already satisfied: testpath in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.6.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.1.2)\n",
      "Requirement already satisfied: bleach in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (4.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (4.11.1)\n",
      "Requirement already satisfied: defusedxml in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.8.4)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->thumb) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install thumb --quiet # This is a tool for evaluation of the potential blog post titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import thumb\n",
    "\n",
    "# Set the environment variables (get your API key: https://platform.openai.com/account/api-keys)\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_API_KEY_HERE\" # optional: for langsmith tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b572387340cc4c83b08027cc8f78e867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\"Unlock the Power of Neural Networks: A Comprehensive Guide for Enhanced Pattern Recognition and Dâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9966922d5c27494db9ecad17489594c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='ð', style=ButtonStyle()), Button(description='ð', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228c35509eeb47b39c27e3c2786bf2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Progress:', max=40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83a76ee80904fb695a4c9c842184758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='ThumbTest: 80615803')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_prompt = f\"Generate a title tag for this blog post on topic: {TOPIC}, Title: {outline_result.title}. The first 200 characters: {''.join(blog_post)[0:200]}\"\n",
    "\n",
    "# Set up a prompt templates for the a/b test:\n",
    "prompt_a = base_prompt + ' ' + \"Write the title in a style of {style}. Make sure that it is SEO friendly.\"\n",
    "prompt_b = base_prompt + ' ' + \"Write the title in a style of {style}. Make sure that is reader friendly. Focus on the reader.\"\n",
    "\n",
    "# Set test cases with different input variables:\n",
    "cases = [\n",
    "  {\"style\": \"business professional\"},\n",
    "  {\"style\": \"b2b marketing\"},\n",
    "  ]\n",
    "\n",
    "# Generate the responses\n",
    "test = thumb.test([prompt_a, prompt_b], cases, runs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
