{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter in your OpenAI API Key:\")\n",
    "\n",
    "# Loader = PyPDFLoader(\"data/principles_of_marketing_book.pdf\")Â # If the file was local\n",
    "loader = PyPDFLoader(\n",
    "    \"https://storage.googleapis.com/strapi_cms_assets/principles_of_marketing_book.pdf\"\n",
    ")\n",
    "raw_documents = loader.load_and_split()\n",
    "print(raw_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just use the first 100 documents for this example:\n",
    "raw_documents = raw_documents[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the marketing principles .pdf, split it into chunks, embed each chunk and load it into the vector store.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "# You can use OpenSource embeddings instead of OpenAIEmbeddings --> embeddings = HuggingFaceEmbeddings()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the license used within the principles of marketing book?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple QA retrieval system using FAISS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining similarity search across a vector database with a chat model, you can easily create a very simple question answering (QA) system. Expanding on the GPT best practices to avoid hallucinations by asking the chat model to only answer using reference text from the database, you can create a simple system that can answer questions about a specific topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "query = \"License used within the principles of marketing book\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Combine all of the docs into a single string:\n",
    "combined_docs = \" \".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Check the length of the combined docs:\n",
    "print(len(combined_docs))\n",
    "\n",
    "# Create the chat model:\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "template = \"\"\"Given the following text, answer the following question.\n",
    "        Question: {question}\n",
    "        \n",
    "        You must follow the following principles:\n",
    "        - You must only answer using the reference text provided.\n",
    "        - If you don't the answer without the reference text, you must return \"I don't know\".\n",
    "        - It is vital that you return the answer with the reference text and not without.\n",
    "\n",
    "        Reference Text: {combined_docs}\"\"\"\n",
    "\n",
    "# Make the template:\n",
    "system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message])\n",
    "\n",
    "# Create the messages:\n",
    "query = \"What is the license used within the principles of marketing book?\"\n",
    "messages = chat_prompt.format_prompt(\n",
    "    question=query, combined_docs=combined_docs\n",
    ").to_messages()\n",
    "\n",
    "# Call the chat model:\n",
    "response = chat(messages)\n",
    "print(response.content)\n",
    "\n",
    "# Alternatively if we try something that the LLM doesn't have access too within the .pdf marketing book:\n",
    "query = \"What is data science?\"\n",
    "messages = chat_prompt.format_prompt(\n",
    "    question=query, combined_docs=combined_docs\n",
    ").to_messages()\n",
    "\n",
    "# Call the chat model:\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible write the above code in a simpler way with _a chain_. For now think of a chain as a _multiple steps that are executed to accomplish a task_. Additionally you will use the vector database as your back end for searching results, commonly referred to as a _retriever._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")\n",
    "qa.invoke({\"query\": \"What is book's title?\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about Document QA retrieval here https://python.langchain.com/docs/modules/chains/additional/question_answering.html.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search with score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS-specific techniques exist, one of which is termed as `.similarity_search_with_score`. This particular method permits the retrieval of not only the corresponding documents but also the distance measure between the query and the said documents. The returned distance measure utilizes L2 distance, where a smaller score is a better match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the license used within the principles of marketing book?\"\n",
    "docs_and_scores = db.similarity_search_with_score(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_and_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to add documents to the FAISS vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = [\n",
    "    Document(\n",
    "        page_content=\"Data engineering begins with data collection. Often, data streams in from multiple sources, such as customer interactions, website activity, social media, IoT devices, and more.\",\n",
    "        metadata={\"title\": \"Data Engineering Book\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Pandas is a popular open-source Python library widely used for data manipulation and analysis. It provides powerful data structures like DataFrames and Series, which allow users to handle and analyze structured data easily. \",\n",
    "        metadata={\"title\": \"Pandas Analysis Book\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.add_documents(documents=new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if you search for pandas you should see the new documents in the results.\n",
    "db.similarity_search_with_score(query=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Saving the Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\n",
    "    \"data/vectorstore\"\n",
    ")  # This creates a index.faiss and index.pkl file in the data/vectorstore directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = FAISS.load_local(\"data/vectorstore\", embeddings)\n",
    "\n",
    "docs = new_db.similarity_search_with_score(query=\"pandas\")\n",
    "\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging two indexes together:\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"I love data engineering\",\n",
    "        metadata={\"title\": \"Data Engineering Book\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I love pandas\",\n",
    "        metadata={\"title\": \"Pandas Analysis Book\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "vectorstore_one = FAISS.from_documents([documents[0]], embeddings)\n",
    "vectorstore_two = FAISS.from_documents([documents[1]], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorstore_one.docstore._dict)\n",
    "print(vectorstore_two.docstore._dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_one.merge_from(vectorstore_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorstore_one.docstore._dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and Similarity Search\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The capability to filter is also available in the FAISS vectorstore. However, since FAISS does not inherently support this feature, it requires manual implementation. This process entails initially retrieving more results than `k`, followed by their filtration. Document filtration can be executed based on metadata. Additionally, you have the option to determine the quantity of documents you wish to fetch prior to filtering by setting the `fetch_k` parameter during any search method invocation. To illustrate, consider the following minor example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Sample list of data engineering documents with content and metadata\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Data engineering involves designing data pipelines.\",\n",
    "        metadata=dict(page=1),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ETL is a key process in data engineering workflows.\",\n",
    "        metadata=dict(page=1),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data modeling ensures data integrity and accessibility.\",\n",
    "        metadata=dict(page=2),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data warehouses are centralized repositories for structured data.\",\n",
    "        metadata=dict(page=2),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Scalability and performance are crucial in data engineering.\",\n",
    "        metadata=dict(page=3),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data lakes store raw data for future processing and analysis.\",\n",
    "        metadata=dict(page=3),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data governance ensures data security and compliance.\",\n",
    "        metadata=dict(page=4),\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Data science and data engineering collaborate for data insights.\",\n",
    "        metadata=dict(page=4),\n",
    "    ),\n",
    "]\n",
    "\n",
    "db = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "results_with_scores = db.similarity_search_with_score(\n",
    "    \"data engineering\", k=2, fetch_k=20, filter={\"page\": 1}\n",
    ")  # Fetch top 20 results and return top 2 results with scores, filtered by page 1\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximal Marginal Relevance (MMR)\n",
    "\n",
    "Maximal Marginal Relevance (MMR) search is an algorithm used to diversify the results obtained from an information retrieval system. It was introduced to address the issue of redundancy in information retrieval results.\n",
    "\n",
    "In the context of generative AI models like GPT-4, an MMR search would strive to balance the trade-off between relevance (how closely the response matches the prompt) and novelty (how different each response is from the others).\n",
    "\n",
    "The MMR algorithm works by iteratively selecting the item that maximizes a certain criterion, typically a weighted combination of relevance to the query and dissimilarity to the already selected items. The main idea is to re-rank the list of items retrieved by an initial search in order to promote diversity.\n",
    "\n",
    "It's possible to use MMR whilst searching on a vector database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.max_marginal_relevance_search(\"data engineering\", filter=dict(page=1))\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
