{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Custom imports:\n",
    "from content_collection import collect_serp_data_and_extract_text_from_webpages\n",
    "from custom_summarize_chain import create_all_summaries, DocumentSummary\n",
    "from expert_interview_chain import InterviewChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-search-results pandas html2text pytest-playwright chromadb nest_asyncio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant variables:\n",
    "TOPIC = \"Neural networks\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\"\n",
    "os.environ[\"STABILITY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract content from webpages into LangChain documents:\n",
    "text_documents = await collect_serp_data_and_extract_text_from_webpages(topic=TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM, text splitter + parser:\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1500, chunk_overlap=400\n",
    ")\n",
    "parser = PydanticOutputParser(pydantic_object=DocumentSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing the data!\n",
      "Summarizing the data!\n",
      "Summarizing the data!\n",
      "Time taken: 3.486579179763794\n",
      "Finished summarizing the data!\n",
      "---\n",
      "Time taken: 3.627920150756836\n",
      "Finished summarizing the data!\n",
      "---\n",
      "Time taken: 37.57901906967163\n",
      "Finished summarizing the data!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "summaries = await create_all_summaries(text_documents, parser, llm, text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Expert Interview Questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_chain = InterviewChain(topic=TOPIC, document_summaries=summaries)\n",
    "interview_questions = interview_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer The Interview Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question: Can you explain the concept of neural networks and how they are related to machine learning?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: What is the role of training data in improving the accuracy of neural networks?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: How do neural networks make complex decisions based on previous outputs?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: What is the significance of sigmoid neurons and supervised learning in training neural networks?\n",
      "\n",
      "------------------------------------------\n",
      "Answer the following question: Can you provide examples of real-world applications where neural networks are used?\n",
      "\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for question in interview_questions.questions:\n",
    "    print(f\"Answer the following question: {question.question}\\n\", flush=True)\n",
    "    answer = input(f\"Answer the following question: {question.question}\\n\")\n",
    "    print('------------------------------------------')\n",
    "    question.answer = answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## General Article Outline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the outline...\n",
      "---\n",
      "Finished generating the outline!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from article_outline_generation import BlogOutlineGenerator\n",
    "\n",
    "blog_outline_generator = BlogOutlineGenerator(topic=TOPIC, questions_and_answers=[item.dict()  \n",
    "                                                                                  for item in interview_questions.questions ] )\n",
    "questions_and_answers = blog_outline_generator.questions_and_answers\n",
    "outline_result = blog_outline_generator.generate_outline(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Article Text Generation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the blog post...\n",
      "---\n",
      "Finished generating the blog post!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from article_generation import ContentGenerator\n",
    "\n",
    "content_gen = ContentGenerator(topic=TOPIC, outline=outline_result, questions_and_answers=questions_and_answers)\n",
    "\n",
    "# Vectorize and store the original webpages:\n",
    "content_gen.split_and_vectorize_documents(text_documents)\n",
    "\n",
    "# Create the blog post (this may take several minutes to run, so please be patient):\n",
    "blog_post = content_gen.generate_blog_post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Neural Networks: An Overview\\n\\n## What are Neural Networks?\\n\\nNeural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. They are computational models inspired by the structure and functioning of the human brain. Neural networks consist of interconnected nodes, or \"neurons,\" organized into layers.\\n\\nA neural network typically has three types of layers:\\n1. Input Layer: This layer receives the initial data or input features. Each neuron in the input layer represents a feature of the data.\\n2. Hidden Layers: These intermediate layers process the input data through a series of weighted connections and apply activation functions to introduce non-linearity. Hidden layers are responsible for learning complex patterns and representations from the data.\\n3. Output Layer: The final layer produces the network\\'s prediction or output. The number of neurons in the output layer depends on the specific task, such as classification, regression, or any other problem.\\n\\nNeural networks operate by propagating information through the network in a forward pass, from the input layer through the hidden layers to the output layer. Each connection between neurons is associated with a weight that determines the strength of the connection. Each neuron in the hidden and output layers calculates a weighted sum of its inputs and applies an activation function to the result. The activation function introduces non-linearity, allowing the network to capture complex patterns and relationships in the data.\\n\\nDuring training, the network learns to adjust its weights and biases to minimize the difference between its predictions and the actual target values in the training data. This process is known as backpropagation and involves gradient-based optimization techniques like gradient descent. The neural network continues to update its weights iteratively, improving its ability to make accurate predictions as it learns from more data.\\n\\nNeural networks are a fundamental concept in machine learning and play a crucial role in many modern machine learning models. They are particularly well-suited for tasks involving complex patterns, such as image recognition, natural language processing, and speech recognition. Neural networks can be used for both supervised learning (where the model learns from labeled data) and unsupervised learning (where the model extracts patterns and structures from unlabeled data).\\n\\nBy leveraging their ability to learn hierarchical representations and sequential dependencies within data, neural networks can make complex decisions based on previous outputs. This is particularly evident in recurrent neural networks (RNNs) and certain types of deep neural networks like transformers.\\n\\nIn summary, neural networks are powerful computational models inspired by the human brain. They excel at learning complex patterns and relationships from data, making them invaluable in various real-world applications. In the following sections, we will explore the role of training data, making complex decisions, the significance of sigmoid neurons and supervised learning, and real-world applications of neural networks.\\n\\nStay tuned for the next sections of this blog post, where we will dive deeper into these fascinating aspects of neural networks.',\n",
       " \"# The Role of Training Data\\n\\nTraining data plays a fundamental role in improving the accuracy of neural networks. It provides the necessary examples for the network to learn and make accurate predictions or decisions. Let's explore the significance of training data in more detail:\\n\\n1. Learning Patterns: Neural networks are designed to identify patterns, relationships, and underlying structures in data. Training data provides the network with examples of these patterns, allowing it to learn and adapt its internal parameters (weights and biases) accordingly.\\n\\n2. Error Minimization: During the training process, the neural network makes predictions based on its current set of parameters. These predictions are compared to the actual target values (ground truth) in the training data. The discrepancies between predictions and ground truth are quantified using a loss function. The network's primary objective is to minimize this loss.\\n\\n3. Gradient Descent: To minimize the loss, neural networks use optimization algorithms like gradient descent. Gradients (derivatives) of the loss with respect to the network's parameters (weights and biases) are calculated. These gradients guide the network in adjusting its parameters in a direction that reduces the loss.\\n\\n4. Iterative Improvement: The training process is iterative, with the network repeatedly adjusting its parameters based on the calculated gradients. By processing the training data multiple times (epochs), the network fine-tunes its parameters to progressively improve its performance. It learns to generalize from the training data to make accurate predictions on new, unseen data.\\n\\n5. Generalization: Through exposure to a diverse set of examples in the training data, neural networks learn to generalize their understanding of patterns. Generalization ensures that the network can apply what it has learned to make accurate predictions on data it hasn't seen before. This is critical for real-world applicability.\\n\\n6. Data Complexity: Training data allows neural networks to handle complex and high-dimensional data. Without sufficient training data, networks may struggle to capture intricate patterns and relationships.\\n\\nThese insights highlight the importance of training data in the training process of neural networks. By providing diverse and representative examples, training data enables neural networks to learn from the patterns in the data and make accurate predictions or decisions.\",\n",
       " '# Making Complex Decisions\\n\\nNeural networks have the remarkable ability to make complex decisions based on previous outputs. This capability is particularly evident in recurrent neural networks (RNNs) and certain types of deep neural networks like transformers. Let\\'s explore how neural networks accomplish this feat:\\n\\n1. Sequential Processing: In many real-world scenarios, decisions are not made in isolation but rather in a sequential manner, where previous outputs and context are crucial for the current decision. Neural networks, particularly RNNs and some types of deep learning models like transformers, are designed to handle sequential data and remember information from previous steps.\\n\\n2. Recurrent Neural Networks (RNNs): RNNs have an internal state, often referred to as a \"hidden state\" or \"memory,\" which is updated at each step of the sequence. The hidden state at time t is influenced by the hidden state at time t-1 and the current input at time t. This recurrent nature allows RNNs to maintain a form of memory about the previous steps in the sequence, making them suitable for tasks like natural language processing, speech recognition, and time-series analysis.\\n\\n3. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU): To address the vanishing gradient problem and improve the ability to capture longer-term dependencies, variations of RNNs like LSTMs and GRUs were introduced. LSTMs and GRUs incorporate mechanisms to selectively forget or update information in the hidden state, enabling them to capture and retain important information over longer sequences.\\n\\n4. Transformer-based Models: Transformer-based models, like the ones used in machine translation and natural language understanding, use a different approach. They employ attention mechanisms to weigh the importance of different parts of the input sequence when making decisions at each step. Transformers can capture complex dependencies between elements in the sequence and have achieved state-of-the-art results in various natural language processing tasks.\\n\\nBy leveraging their ability to learn hierarchical representations and sequential dependencies within data, neural networks can make sophisticated decisions based on previous outputs. This enables them to tackle complex tasks such as language translation, speech recognition, and even playing strategic games like chess or Go.\\n\\nThese advancements in neural network architectures have opened up new possibilities for solving complex problems and have contributed to the rapid progress in fields like natural language processing, computer vision, and robotics.\\n\\nIn the next section, we will explore the significance of sigmoid neurons and supervised learning in training neural networks.',\n",
       " \"# Significance of Sigmoid Neurons and Supervised Learning\\n\\nIn the training and functioning of neural networks, sigmoid neurons and supervised learning play a significant role. Let's explore their significance in more detail:\\n\\n1. Sigmoid Neurons:\\n   - **Activation Function**: Sigmoid neurons use the sigmoid activation function, which introduces non-linearity into the neural network. This non-linearity enables neural networks to capture complex, nonlinear relationships in the data. Without non-linear activation functions like sigmoid, neural networks would be limited to linear transformations, making them less capable of modeling intricate patterns.\\n   - **Smooth Gradients**: Sigmoid functions have smooth derivatives, making them well-suited for gradient-based optimization algorithms like gradient descent. The smoothness of the gradient facilitates efficient weight updates during training, leading to quicker convergence and improved training stability.\\n   - **Output Range**: Sigmoid neurons produce outputs in the range of (0, 1), which can be interpreted as probabilities. This property is particularly valuable in binary classification tasks, where the network's output can be seen as the probability of belonging to a specific class.\\n\\n2. Supervised Learning:\\n   - **Training Signal**: Supervised learning involves training neural networks on labeled data, where each data point is associated with a known target or ground truth. This labeled data provides a clear training signal, allowing the network to compare its predictions to the correct answers and adjust its parameters accordingly. It enables the network to learn from its mistakes.\\n   - **Error Minimization**: In supervised learning, the primary objective is to minimize the difference between the network's predictions and the actual target values. This is typically quantified using a loss function, such as mean squared error (MSE). By iteratively updating the network's parameters through techniques like backpropagation and gradient descent, the network learns to make more accurate predictions and reduce the overall error.\\n\\nThe significance of sigmoid neurons and supervised learning lies in their ability to enhance the capabilities of neural networks. Sigmoid neurons introduce non-linearity, enabling the network to model complex relationships and make more sophisticated decisions. Supervised learning with labeled data provides a clear training signal for the network to learn from, allowing it to improve its predictions over time.\\n\\nThese concepts are essential for training neural networks and have contributed to their success in various real-world applications. In the next section, we will explore some of these real-world applications where neural networks are used.\",\n",
       " \"# Real-World Applications of Neural Networks\\n\\nNeural networks have found applications in various domains, showcasing their versatility and power in solving complex problems. Let's explore some of the real-world applications where neural networks are used:\\n\\n1. Image and Video Processing:\\n   - **Image Classification**: Neural networks are used for classifying objects and scenes in images. For example, they can identify cats in photographs.\\n   - **Object Detection**: Neural networks are employed in security cameras and self-driving cars to detect and track objects like pedestrians, vehicles, and road signs.\\n   - **Facial Recognition**: Neural networks power facial recognition systems in smartphones, airports, and security systems.\\n   - **Video Analysis**: Neural networks enable video analysis for surveillance, content recommendation, and even deepfake detection.\\n\\n2. Natural Language Processing (NLP):\\n   - **Machine Translation**: Neural networks, especially transformer models, have revolutionized machine translation, as seen in applications like Google Translate.\\n   - **Sentiment Analysis**: Neural networks are used to analyze social media posts and customer reviews to gauge public sentiment about products, services, or events.\\n   - **Chatbots**: Virtual assistants like Siri, Alexa, and chatbots on websites use neural networks for natural language understanding and conversation.\\n\\n3. Healthcare:\\n   - **Medical Imaging**: Neural networks assist in diagnosing diseases by analyzing medical images such as X-rays, MRIs, and CT scans.\\n   - **Drug Discovery**: Neural networks are employed in drug discovery processes to predict potential drug candidates and assess their efficacy.\\n   - **Patient Risk Assessment**: Neural networks analyze patient data to assess health risks and predict disease outcomes.\\n\\n4. Financial Services:\\n   - **Credit Scoring**: Neural networks can analyze customer data to assess creditworthiness and provide credit scoring.\\n   - **Fraud Detection**: Neural networks are used to detect fraudulent transactions by learning patterns of fraudulent behavior.\\n   - **Stock Market Prediction**: Neural networks can analyze historical stock data to predict future market trends and make investment decisions.\\n\\n5. Autonomous Vehicles:\\n   - **Self-Driving Cars**: Neural networks play a vital role in self-driving cars by processing sensor data, identifying objects, and making real-time driving decisions.\\n   - **Traffic Control**: Neural networks can optimize traffic flow by analyzing real-time traffic data and predicting congestion patterns.\\n\\nThese are just a few examples of the wide range of applications where neural networks are making a significant impact. Their ability to learn from data and make accurate predictions has revolutionized industries and opened up new possibilities for solving complex problems.\\n\\nStay tuned for the next section of this blog post, where we will wrap up our discussion on neural networks and provide some concluding thoughts.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Image Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_generation_chain import create_image\n",
    "image = create_image(outline_result.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is ready! The filepath is c31daed.png\n"
     ]
    }
   ],
   "source": [
    "print(f\"The image is ready! The filepath is {image[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
